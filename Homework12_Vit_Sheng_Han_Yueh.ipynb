{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader, random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27afa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# 1. Build ViT model using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7841e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size = 16, emb_size = 768, img_size = 224):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.pro = nn.Sequential(\n",
    "            nn.Conv2d(3, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        self.token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        \n",
    "        # Positional embeddings\n",
    "        self.pos = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        x = self.pro(x)\n",
    "        tokens = repeat(self.token, '() n e -> b n e', b=b)\n",
    "    \n",
    "        x = torch.cat([tokens, x], dim=1)\n",
    "        \n",
    "        x += self.pos\n",
    "        return x\n",
    "    \n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8):\n",
    "        super(MHA, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model*3)\n",
    "        self.projection = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        queries, keys, values = [t.reshape(t.shape[0], -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2) for t in qkv]\n",
    "        E = torch.matmul(queries, keys.transpose(-2, -1))\n",
    "        \n",
    "        if mask:\n",
    "            fv = torch.finfo(torch.float32).min\n",
    "            E.mask_fill(~mask, fv)\n",
    "            \n",
    "        \n",
    "        attn = F.softmax(E, dim=-1)/self.d_model**0.5\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', attn, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "    \n",
    "class FFB(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion= 4):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,emb_size= 768,forward_expansion = 4,** kwargs):\n",
    "        super().__init__(\n",
    "            Residual(nn.Sequential(nn.LayerNorm(emb_size),MHA(emb_size, **kwargs),nn.Dropout(0.2))),\n",
    "            Residual(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FFB(emb_size, expansion=forward_expansion),\n",
    "                nn.Dropout(0.2)\n",
    "            )\n",
    "            ))\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, emb_size=768, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.reduce = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(emb_size)\n",
    "        self.fc = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reduce(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "        \n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,patch_size = 16,emb_size= 768,img_size= 224,depth= 8,n_classes = 4,**kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5871ec8d",
   "metadata": {},
   "source": [
    "* Ref:   \n",
    "https://www.youtube.com/watch?v=HZ4j_U3FC94&t=4s&ab_channel=ShusenWang  \n",
    "https://github.com/FrancescoSaverioZuppichini/ViT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c45b84",
   "metadata": {},
   "source": [
    "# 2. Download the [cow teat datasets](https://github.com/YoushanZhang/SCTL) and train your model using cow teat datasets (you may need to use  Google Colab (or Kaggle) with GPU to train your code) \n",
    "\n",
    "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
    "### (2) use custom dataloader for test dataset (return image tensor and file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03445785",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'D:\\application for the usa\\YU\\6. Deep learning\\HW 6 NN for cow teats classification\\Train'\n",
    "test_dir = r'D:\\application for the usa\\YU\\6. Deep learning\\HW 6 NN for cow teats classification\\Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f55d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "transform = transforms.Compose([transforms.Resize((275, 275)),\n",
    "                                    transforms.RandomRotation((-20,20)),\n",
    "                                    transforms.RandomCrop((224, 224)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e541eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.Resize((275,275)),\n",
    "                                    transforms.RandomCrop((224, 224)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50effdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSet(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels[idx])\n",
    "        image = Image.open(img_path) \n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7ee1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageFolder(train_dir,transform = transform)\n",
    "\n",
    "\n",
    " # Random split\n",
    "train_set_size = int(len(train_data) * 0.8)\n",
    "valid_set_size = len(train_data) - train_set_size\n",
    "train_set, valid_set = random_split(train_data, [train_set_size, valid_set_size])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TestSet(img_dir = test_dir, transform = test_transform)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86e1eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT().to('cuda')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46320c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Train Loss: 1.0400133165976275| train_acc:56.25680087051143 | Validation Loss:0.9425189819024957 | val_acc:59.130434782608695\n",
      "\n",
      "Epoch:2 | Train Loss: 0.9844639234244823| train_acc:57.562568008705114 | Validation Loss:0.9296859111474908 | val_acc:57.391304347826086\n",
      "\n",
      "Epoch:3 | Train Loss: 0.974345791955357| train_acc:57.018498367791075 | Validation Loss:0.9214377307373545 | val_acc:60.869565217391305\n",
      "\n",
      "Epoch:4 | Train Loss: 0.9647849333675012| train_acc:57.99782372143634 | Validation Loss:0.9092619514983633 | val_acc:63.04347826086956\n",
      "\n",
      "Epoch:5 | Train Loss: 0.9663013966835063| train_acc:58.54189336235038 | Validation Loss:0.9080799654774044 | val_acc:62.17391304347826\n",
      "\n",
      "Epoch:6 | Train Loss: 0.9589415117450383| train_acc:57.12731229597389 | Validation Loss:0.9021669053513071 | val_acc:64.78260869565217\n",
      "\n",
      "Epoch:7 | Train Loss: 0.9549827663794808| train_acc:57.78019586507073 | Validation Loss:0.9065804090188897 | val_acc:62.17391304347826\n",
      "\n",
      "Epoch:8 | Train Loss: 0.9463991280807101| train_acc:58.759521218715996 | Validation Loss:0.8979544390802798 | val_acc:66.08695652173913\n",
      "\n",
      "Epoch:9 | Train Loss: 0.9485747965781585| train_acc:59.08596300326442 | Validation Loss:0.8917650756628617 | val_acc:63.04347826086956\n",
      "\n",
      "Epoch:10 | Train Loss: 0.943112892171611| train_acc:59.95647442872688 | Validation Loss:0.8982012559538303 | val_acc:63.47826086956522\n",
      "\n",
      "Epoch:11 | Train Loss: 0.9359654236098994| train_acc:59.19477693144722 | Validation Loss:0.9004811717116314 | val_acc:63.04347826086956\n",
      "\n",
      "Epoch:12 | Train Loss: 0.9367160175643537| train_acc:59.630032644178456 | Validation Loss:0.893801620732183 | val_acc:60.869565217391305\n",
      "\n",
      "Epoch:13 | Train Loss: 0.9394333797304527| train_acc:59.19477693144722 | Validation Loss:0.8963496840518453 | val_acc:65.65217391304348\n",
      "\n",
      "Epoch:14 | Train Loss: 0.9273509803673495| train_acc:60.174102285092495 | Validation Loss:0.9079529707846434 | val_acc:64.34782608695652\n",
      "\n",
      "Epoch:15 | Train Loss: 0.9257394072802171| train_acc:60.2829162132753 | Validation Loss:0.8799612711305204 | val_acc:65.65217391304348\n",
      "\n",
      "Epoch:16 | Train Loss: 0.9220022083300611| train_acc:59.41240478781284 | Validation Loss:0.8905604624229929 | val_acc:62.608695652173914\n",
      "\n",
      "Epoch:17 | Train Loss: 0.9229564368400884| train_acc:60.50054406964092 | Validation Loss:0.8904870673366215 | val_acc:61.73913043478261\n",
      "\n",
      "Epoch:18 | Train Loss: 0.923036210724841| train_acc:59.41240478781284 | Validation Loss:0.9164867660273677 | val_acc:61.30434782608695\n",
      "\n",
      "Epoch:19 | Train Loss: 0.9166561866260093| train_acc:60.174102285092495 | Validation Loss:0.8995066839715709 | val_acc:63.04347826086956\n",
      "\n",
      "Epoch:20 | Train Loss: 0.9209535177311171| train_acc:59.521218715995644 | Validation Loss:0.9025497472804526 | val_acc:63.47826086956522\n",
      "\n",
      "Duration: 0:11:30.256633\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "all_trainloss= []\n",
    "all_valloss= []\n",
    "all_acc = []\n",
    "all_train_acc=[]\n",
    "steps = 20\n",
    "\n",
    "for epoch in range(steps):\n",
    "    train_loss = 0\n",
    "    train_correct=0\n",
    "    for batch in train_loader:\n",
    "        x, y = batch\n",
    "\n",
    "        x, y = x.to('cuda'), y.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)      \n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        train_loss+=loss.item()   \n",
    "        predict = pred.argmax(dim=1, keepdim=True)\n",
    "        train_correct += predict.eq(y.view_as(predict)).sum().item()\n",
    "        \n",
    "    train_acc = 100. * train_correct / len(train_loader.dataset)        \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "        output = model(data)\n",
    "        val_loss += criterion(output, target).item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "    train_loss/=len(train_loader)\n",
    "    val_loss /= len(valid_loader)\n",
    "    accuracy = 100. * correct / len(valid_loader.dataset)\n",
    "    \n",
    "    #scheduler.step(val_loss)\n",
    "    \n",
    "    if accuracy >= 60 or (epoch+1)%100 == 0:\n",
    "        PATH = f'Vit_{epoch+1}.pth'\n",
    "        torch.save(model.state_dict(), PATH)        \n",
    "    \n",
    "    all_trainloss.append(train_loss)\n",
    "    all_train_acc.append(train_acc)    \n",
    "    all_valloss.append(val_loss)\n",
    "    all_acc.append(accuracy)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch:{epoch+1} | Train Loss: {train_loss}| train_acc:{train_acc} | Validation Loss:{val_loss} | val_acc:{accuracy}')\n",
    "    print()\n",
    "        \n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9212c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip(all_trainloss,all_train_acc,all_valloss,all_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3. Evaluate your model using the developed software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687038bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = (df[df.iloc[:,3]>=60].index.values)+1\n",
    "for i in index:\n",
    "\n",
    "    PATH = f'Vit_{i}.pth'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    \n",
    "    CNN_pred=[]\n",
    "    test_y=[]\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            x, y = data\n",
    "            #print(y)\n",
    "            x = x.to('cuda')\n",
    "            outputs = model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            CNN_pred+=predicted.tolist()\n",
    "            test_y+= list(y)\n",
    "    res =  pd.DataFrame(list(zip(test_y, CNN_pred)))\n",
    "    res.to_csv(f'Vit_{i}_.csv',index=False,header=False)"
   ]
  },
  {
   "attachments": {
    "Screenshot%202023-04-26%20100400.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a5314b88",
   "metadata": {},
   "source": [
    "![Screenshot%202023-04-26%20100400.png](attachment:Screenshot%202023-04-26%20100400.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f5074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
