{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e63eeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats as st\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be843341",
   "metadata": {},
   "source": [
    "# 1. Implement Decision Tree method (50 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d0898ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, criteria=None, left=None, right=None, value=None):\n",
    "        self.feature=feature\n",
    "        self.criteria = criteria\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None\n",
    "        \n",
    "class Decision_Tree:\n",
    "    def __init__(self, min_sample_split=2, max_depth=100, n_feature= None):\n",
    "        self.min_sample_split=min_sample_split\n",
    "        self.max_depth=max_depth\n",
    "        self.n_feature=n_feature\n",
    "        self.root=None\n",
    "    \n",
    "    \n",
    "    def fit(self, train_x, train_y):\n",
    "        if not self.n_feature:\n",
    "            self.n_feature=train_x.shape[1]\n",
    "        else:\n",
    "            self.n_feature=min(train_x.shape[1],self.n_feature)\n",
    "        self.root=self.grow(train_x, train_y)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def grow(self, train_x, train_y, depth=0):\n",
    "        n, m = train_x.shape\n",
    "        n_class = len(np.unique(train_y))\n",
    "        \n",
    "        # check stoppping criteria\n",
    "        if depth > self.max_depth or n<self.min_sample_split or n_class ==1:\n",
    "            \n",
    "            #counter = Counter(train_y.to_list())\n",
    "            \n",
    "            #val = counter.most_common(1)[0][0] \n",
    "            #print(train_y)\n",
    "            leaf_val = st.mode(train_y.reshape(-1).tolist())[0][0]\n",
    "            #leaf_val=0\n",
    "            return Node(value=leaf_val)\n",
    "        feature_idx = np.random.choice(m, self.n_feature, replace = False)\n",
    "        \n",
    "        # find the best split\n",
    "        best_feat, best_thre= self.best_split(train_x, train_y, feature_idx)[:-1]\n",
    "        \n",
    "        # create child node\n",
    "        left_idx = np.argwhere(train_x[:, best_feat]<= best_thre).flatten()\n",
    "        right_idx = np.argwhere(train_x[:, best_feat]> best_thre).flatten()\n",
    "        left = self.grow(train_x[left_idx,:], train_y[left_idx], depth+1)\n",
    "        right = self.grow(train_x[right_idx,:], train_y[right_idx], depth+1)\n",
    "        return Node(best_feat, best_thre, left, right)\n",
    "        \n",
    "    def entropy(self, y):\n",
    "        _,counts = np.unique(y, return_counts=True)\n",
    "        #print('counts:',counts)\n",
    "        prob = counts/len(y)\n",
    "        return -np.sum(prob*np.log2(prob))\n",
    "    \n",
    "    def best_split(self, train_x, train_y, feature_idx):\n",
    "        min_en_list=[]\n",
    "        best=-np.inf\n",
    "        split_idx=None\n",
    "        split_criteria = None\n",
    "        for idx in feature_idx:\n",
    "            min_en = np.inf\n",
    "            xi = train_x[:,idx]\n",
    "            options = np.sort(np.unique(xi))\n",
    "            #print(options.shape)\n",
    "            for opt in options:\n",
    "                #print(xi, train_y,opt)\n",
    "                ig, en = self.information_gain(xi, train_y, opt)\n",
    "                #print('opt:',opt,'\\n', ig, '\\n',en)\n",
    "                \n",
    "                if en < min_en:\n",
    "                    min_en = en\n",
    "                if ig > best:\n",
    "                    best = ig\n",
    "                    split_idx = idx\n",
    "                    split_criteria = opt\n",
    "            min_en_list.append(min_en)\n",
    "               \n",
    "        return split_idx, split_criteria, min_en_list\n",
    "        \n",
    "    def information_gain(self, xi, train_y, opt):\n",
    "        # Calculate parents entropy\n",
    "        parent_entropy = self.entropy(train_y)\n",
    "        \n",
    "        # Calculate weights*childrent entropy\n",
    "        left_idx = np.argwhere(xi<=opt).flatten()\n",
    "        #print('left_idx',left_idx)\n",
    "        right_idx = np.argwhere(xi>opt).flatten()\n",
    "        #print('right_idx',right_idx)\n",
    "        if len(left_idx)==0 or len(right_idx)==0:\n",
    "            \n",
    "            return 0, np.inf\n",
    "        \n",
    "        n = len(train_y)\n",
    "        nl, nr = len(left_idx), len(right_idx)\n",
    "        total = nl+nr\n",
    "        left_entropy = self.entropy(train_y[left_idx])\n",
    "        right_entropy = self.entropy(train_y[right_idx])\n",
    "        #print('left_e:',left_entropy)\n",
    "        #print('right_e:',right_entropy)\n",
    "        children_entropy = (nl/total)*left_entropy + (nr/total)*right_entropy\n",
    "        ig = parent_entropy-children_entropy\n",
    "        \n",
    "        return ig, children_entropy\n",
    "        \n",
    "    \n",
    "    def predict(self, test_x):\n",
    "        res=[]\n",
    "        for x in test_x:\n",
    "            res.append(self.traverse(x, self.root))\n",
    "        return np.array(res)\n",
    "        \n",
    "    def traverse(self, x, node):\n",
    "        if node.is_leaf_node():\n",
    "            return node.value\n",
    "        if x[node.feature]<=node.criteria:\n",
    "            return self.traverse(x, node.left)\n",
    "        return self.traverse(x, node.right)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f6d4f",
   "metadata": {},
   "source": [
    "* Ref:  \n",
    "1. https://youtu.be/NxEHSAfFlK8  \n",
    "2. https://anderfernandez.com/en/blog/code-decision-tree-python-from-scratch/  \n",
    "3. https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29915904",
   "metadata": {},
   "source": [
    "# 2. Load train and test mat files, perform Decision Tree and report acuracy on the test dataset (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2eb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = scipy.io.loadmat('train.mat')\n",
    "test = scipy.io.loadmat('test.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85072456",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['features']\n",
    "train_y = train['labels']\n",
    "test_x = test['features']\n",
    "test_y = test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "543dba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, test_y):\n",
    "    return np.sum(pred==test_y)/len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbd7c51",
   "metadata": {},
   "source": [
    "### Fit all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811c602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decision_Tree()\n",
    "model.fit(train_x, train_y)\n",
    "pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4ddc742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 96.16 %\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',round(accuracy(pred, test_y),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6890b0",
   "metadata": {},
   "source": [
    "### Fit the 200 features that has the lowest entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442c11cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Decision_Tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d579e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_en_list = model.best_split(train_x, train_y, [i for i in range(train_x.shape[1])])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfe5e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.07 %\n"
     ]
    }
   ],
   "source": [
    "# select 200 lowest entropy to be the features\n",
    "low_en_idx = np.array(min_en_list).argsort()[:200]\n",
    "model.fit(train_x[:,low_en_idx], train_y)\n",
    "pred = model.predict(test_x)\n",
    "print('accuracy:',round(accuracy(pred, test_y),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e6aa8",
   "metadata": {},
   "source": [
    "# 3. Reduce the dimensionality of features using PCA to low dimensions (e.g., 10, 3 ) and report the accuracy of the test datasets (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89366431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy in 3 dimensions: 95.86 %\n",
      "accuracy in 10 dimensions: 96.13 %\n",
      "accuracy in 20 dimensions: 95.03 %\n",
      "accuracy in 40 dimensions: 95.38 %\n",
      "accuracy in 60 dimensions: 95.7 %\n",
      "accuracy in 100 dimensions: 95.75 %\n",
      "accuracy in 200 dimensions: 95.7 %\n"
     ]
    }
   ],
   "source": [
    "for i in [3,10,20,40,60,100,200]:\n",
    "    pca = PCA(n_components=i)\n",
    "    reduced_train = pca.fit_transform(train_x)\n",
    "    #print(reduced_train.shape)\n",
    "    reduced_test = pca.fit_transform(test_x)\n",
    "    model.fit(reduced_train, train_y)\n",
    "    pred = model.predict(reduced_test)\n",
    "    print(f'accuracy in {i} dimensions:',round(accuracy(pred, test_y),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31455a2e",
   "metadata": {},
   "source": [
    "# 4. Compare results with your KNN model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cad5b",
   "metadata": {},
   "source": [
    "The selected features based on entropy has the highest predicted accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70101bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
